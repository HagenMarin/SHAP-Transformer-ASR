{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a8e2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hagen/SHAP-Transformer-ASR/.shaptransformerasr/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-12 22:13:08.539950: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-12 22:13:08.696926: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757707988.759424    3720 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757707988.776820    3720 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1757707988.906050    3720 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757707988.906073    3720 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757707988.906074    3720 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757707988.906076    3720 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-12 22:13:08.922078: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import logging\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import shap\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler('evaluation.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_name = \"facebook/wav2vec2-base-960h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "944233e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-12 22:13:11,616 - INFO - Loading model: facebook/wav2vec2-base-960h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-12 22:13:13,185 - INFO - Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Loading model: {model_name}\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "logger.info(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b33fa99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrograms(S_original, S_amplified, sr):\n",
    "    \"\"\"\n",
    "    Visualizes the original and amplified mel spectrograms side by side.\n",
    "    \"\"\"\n",
    "    # Convert power spectrograms to decibels (dB) for better visualization\n",
    "    S_db_original = librosa.power_to_db(S_original, ref=np.max)\n",
    "    S_db_amplified = librosa.power_to_db(S_amplified, ref=np.max)\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(12, 8))\n",
    "    fig.suptitle('Mel Spectrogram Comparison', fontsize=16)\n",
    "\n",
    "    # Plot original spectrogram\n",
    "    img1 = librosa.display.specshow(S_db_original, sr=sr, x_axis='time', y_axis='mel', ax=ax[0])\n",
    "    ax[0].set(title='Original Spectrogram')\n",
    "    ax[0].label_outer() # Hide x-axis label for the top plot\n",
    "    fig.colorbar(img1, ax=ax[0], format='%+2.0f dB')\n",
    "\n",
    "    # Plot amplified spectrogram\n",
    "    img2 = librosa.display.specshow(S_db_amplified, sr=sr, x_axis='time', y_axis='mel', ax=ax[1])\n",
    "    ax[1].set(title='Spectrogram with Amplified Quiet Sections')\n",
    "    fig.colorbar(img2, ax=ax[1], format='%+2.0f dB')\n",
    "\n",
    "    # Adjust layout and display the plot\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust for suptitle\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12202b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_waveforms(y_original, y_modified, sr):\n",
    "    \"\"\"\n",
    "    Visualizes the original and a modified audio waveform side by side.\n",
    "\n",
    "    Args:\n",
    "        y_original (np.ndarray): The original audio time series.\n",
    "        y_modified (np.ndarray): The modified audio time series.\n",
    "        sr (int): The sampling rate of the audio.\n",
    "    \"\"\"\n",
    "    # Create a time array for the x-axis\n",
    "    time_original = librosa.times_like(y_original, sr=sr)\n",
    "    time_modified = librosa.times_like(y_modified, sr=sr)\n",
    "\n",
    "    # Create a figure with two subplots, sharing the x and y axes\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=1, sharex=True, sharey=True, figsize=(12, 8))\n",
    "    fig.suptitle('Waveform Comparison', fontsize=16)\n",
    "\n",
    "    # Plot the original waveform\n",
    "    librosa.display.waveshow(y_original, sr=sr, ax=ax[0], color='b')\n",
    "    ax[0].set(title='Original Waveform')\n",
    "    ax[0].set_ylabel('Amplitude')\n",
    "    ax[0].grid(True, linestyle='--', alpha=0.6)\n",
    "    ax[0].label_outer() # Hide x-axis label for the top plot\n",
    "\n",
    "    # Plot the modified waveform\n",
    "    librosa.display.waveshow(y_modified, sr=sr, ax=ax[1], color='r')\n",
    "    ax[1].set(title='Modified Waveform')\n",
    "    ax[1].set_xlabel('Time (s)')\n",
    "    ax[1].set_ylabel('Amplitude')\n",
    "    ax[1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Add a legend to distinguish the waveforms\n",
    "    fig.legend(['Original', 'Modified'], loc='upper right')\n",
    "    \n",
    "    # Adjust layout to prevent titles from overlapping and display the plot\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust for suptitle\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c97c5d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_scale_shap(shap_values, min_val=0.8, default=0.4):\n",
    "    \"\"\"\n",
    "    Normalize SHAP values to [0, 1] and scale to [min_val, max_val].\n",
    "    \"\"\"\n",
    "    # Normalize to [0, 1]\n",
    "    shap_min = np.min(shap_values)\n",
    "    shap_max = np.max(shap_values)\n",
    "    normalized_shap = (shap_values - shap_min) / (shap_max - shap_min + 1e-8)  # Add small epsilon to avoid division by zero\n",
    "\n",
    "    # Scale to [min_val, max_val]\n",
    "    scaled_shap = ((normalized_shap-min_val).clip(0,1)/(1 - min_val)).clip(default, 1)\n",
    "    return scaled_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f49aff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-12 22:39:48,829 - INFO - Transcription: ['ON THE GENERAL PRINCIPLES OF ART MISTER QUILTER WRITES WITH EQUAL LUCIDITY']\n",
      "2025-09-12 22:39:48,829 - INFO - Ouptut shape: torch.Size([1, 281])\n"
     ]
    }
   ],
   "source": [
    "audio = np.load(\"data/audio_sample_14_clean_inf.npy\")\n",
    "inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "input_values = inputs.input_values.to(device)\n",
    "\n",
    "# retrieve logits\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "logger.info(f\"Transcription: {transcription}\")\n",
    "logger.info(f\"Ouptut shape: {predicted_ids.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ee6e5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-12 22:40:05,808 - INFO - Loaded SHAP values with shape: (90240, 281)\n",
      "2025-09-12 22:40:05,809 - INFO - Loaded audio with shape: (90240,)\n"
     ]
    }
   ],
   "source": [
    "shap_values = np.load(\"data/shap_values_sample_14_clean_inf.npy\")\n",
    "shap_values = shap_values.reshape(audio.shape[0], logits.shape[1])\n",
    "logger.info(f\"Loaded SHAP values with shape: {shap_values.shape}\")\n",
    "logger.info(f\"Loaded audio with shape: {audio.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55026ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m character_shap_values = [np.zeros((\u001b[32m93680\u001b[39m,)) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m characters]\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, char \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(characters):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, timestep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mshap_values\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m):\n\u001b[32m     15\u001b[39m         character_shap_values[idx][i] = \u001b[38;5;28mabs\u001b[39m(timestep[char])\n\u001b[32m     17\u001b[39m window_length_ms = \u001b[32m20\u001b[39m\n",
      "\u001b[31mIndexError\u001b[39m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "characters = [28,  29,  31,  32,  33,  35,  37,  39,  40,  41,  43,  45,  48,  49,\n",
    "         50,  53,  57,  58,  59,  61,  62,  63,  66,  67,  68,  69,  70,  71,\n",
    "         72,  73,  74,  75,  76,  77,  80,  84,  90,  91,  92,  95,  96,  98,\n",
    "         99, 100, 101, 102, 103, 104, 105, 106, 109, 110, 112, 113, 114, 115,\n",
    "        116, 117, 118, 119, 120, 121, 122, 123, 126, 127, 128, 129, 130, 131,\n",
    "        132, 133, 136, 142, 143, 147, 152, 154, 155, 156, 157, 158, 168, 169,\n",
    "        170, 171, 172, 173, 174, 177, 179, 181, 182, 183, 184, 185, 186, 187,\n",
    "        189, 195, 199, 201, 202, 203, 205, 207, 208, 209, 210, 212, 215, 217,\n",
    "        221, 224, 226, 227, 228, 229, 230, 231, 232, 234, 237, 238, 239, 241,\n",
    "        242, 243, 245, 251, 252, 253, 254, 258, 262, 263, 265, 266, 268, 269,\n",
    "        270, 271]\n",
    "character_shap_values = [np.zeros((93680,)) for _ in characters]\n",
    "for idx, char in enumerate(characters):\n",
    "    for i, timestep in enumerate(shap_values[1][0]):\n",
    "        character_shap_values[idx][i] = abs(timestep[char])\n",
    "\n",
    "window_length_ms = 20\n",
    "\n",
    "num_of_frames = window_length_ms*16\n",
    "\n",
    "for arr in character_shap_values:\n",
    "    for idx in range(0,len(arr),num_of_frames):\n",
    "        mean = np.mean(arr[idx:min(idx+num_of_frames,len(arr))])\n",
    "        arr[idx:min(idx+num_of_frames,len(arr))] = mean\n",
    "\n",
    "character_norm_shap_values = [normalize_and_scale_shap(vals,0.80,0.0) for vals in character_shap_values]\n",
    "\n",
    "#print(norm_m_shap_values.shape)\n",
    "\n",
    "character_amplified_audio = [audio * vals for vals in character_norm_shap_values]\n",
    "\n",
    "amplified_audio = sum(character_amplified_audio)/len(character_amplified_audio) * 10\n",
    "S_amplified_audio = librosa.feature.melspectrogram(y=amplified_audio, sr=16000, n_fft=2048, hop_length=512)\n",
    "S_audio = librosa.feature.melspectrogram(y=audio, sr=16000, n_fft=2048, hop_length=512)\n",
    "sf.write(\"m_amplified_audio.wav\", amplified_audio, 16000)\n",
    "plot_spectrograms(S_audio, S_amplified_audio, sr=16000)\n",
    "plot_waveforms(audio, amplified_audio*5, sr=16000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
