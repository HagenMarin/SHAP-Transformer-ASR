{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d960d8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hagen/SHAP-Transformer-ASR/.shaptransformerasr/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-15 13:03:36.973818: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-15 13:03:36.983051: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757934216.992809   29746 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757934216.995646   29746 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1757934217.004638   29746 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757934217.004651   29746 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757934217.004652   29746 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757934217.004653   29746 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-15 13:03:37.007696: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import soundfile as sf\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from datasets import load_dataset\n",
    "import librosa\n",
    "import librosa.display\n",
    "from scipy.stats import pearsonr\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler('evaluation.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20f82024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_model_wrapper(model):\n",
    "    \"\"\"Create a wrapper class for the model to get properly shaped output\"\"\"\n",
    "    class ModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # Ensure input has correct shape for the model\n",
    "            if len(x.shape) == 4:\n",
    "                x = x.squeeze(1).squeeze(1)  # Remove extra dimensions\n",
    "            elif len(x.shape) == 3:\n",
    "                x = x.squeeze(1)  # Remove extra dimension\n",
    "            \n",
    "            # Add attention mask\n",
    "            attention_mask = torch.ones_like(x)\n",
    "            \n",
    "            # Forward pass with attention mask\n",
    "            logits = self.model(x, attention_mask=attention_mask).logits\n",
    "            \n",
    "            # Log the shape and statistics of logits\n",
    "            logger.debug(f\"Logits shape: {logits.shape}\")\n",
    "            logger.debug(f\"Logits mean: {torch.mean(logits).item():.6f}\")\n",
    "            logger.debug(f\"Logits std: {torch.std(logits).item():.6f}\")\n",
    "            \n",
    "            # For SHAP, aggregate over vocab to get a scalar per time step\n",
    "            return torch.max(logits,dim=-1).values  # [batch, seq_len]\n",
    "    \n",
    "    return ModelWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4548d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_noise(audio: np.ndarray, snr_db: float) -> np.ndarray:\n",
    "    \"\"\"Add white noise to audio at specified SNR\"\"\"\n",
    "    signal_power = np.mean(audio ** 2)\n",
    "    noise_power = signal_power / (10 ** (snr_db / 10))\n",
    "    noise = np.random.normal(0, np.sqrt(noise_power), len(audio))\n",
    "    return audio + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e58bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_set(num_samples: int = 10) -> Dict:\n",
    "    \"\"\"Create a controlled test set with various conditions\"\"\"\n",
    "    logger.info(f\"Creating test set with {num_samples} samples\")\n",
    "    ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "    test_set = []\n",
    "    dataset_index = 15\n",
    "\n",
    "    for i in tqdm(range(min(num_samples, len(ds))), desc=\"Creating test samples\"):\n",
    "        sample = ds[i+dataset_index]\n",
    "        audio = sample[\"audio\"][\"array\"]\n",
    "        while len(audio) < 100000:\n",
    "            dataset_index += 1\n",
    "            sample = ds[i+dataset_index]\n",
    "            audio = sample[\"audio\"][\"array\"]\n",
    "        text = sample[\"text\"]\n",
    "        \n",
    "        # Create clean sample\n",
    "        test_set.append({\n",
    "            \"type\": \"clean\",\n",
    "            \"audio\": audio,\n",
    "            \"text\": text,\n",
    "            \"snr\": float('inf'),\n",
    "            \"noise\": np.zeros_like(audio)\n",
    "        })\n",
    "        logger.info(f\"Added clean sample {i+1}\")\n",
    "        \n",
    "        # Create noisy samples with different SNRs [20, 10, 0, -5]\n",
    "        for snr in tqdm([5,2,1], desc=f\"Adding noise to sample {i+1}\", leave=False):\n",
    "            noisy_audio = _add_noise(audio, snr)\n",
    "            test_set.append({\n",
    "                \"type\": \"noisy\",\n",
    "                \"audio\": noisy_audio,\n",
    "                \"text\": text,\n",
    "                \"snr\": snr,\n",
    "                \"noise\": noisy_audio - audio\n",
    "            })\n",
    "            logger.info(f\"Added noisy sample {i+1} with SNR {snr}dB\")\n",
    "\n",
    "    logger.info(f\"Test set created with {len(test_set)} total samples\")\n",
    "    return test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b0c5a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_shap_values(processor, device, wrapped_model, model, vocab, audio: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Compute SHAP values for an audio sample using GradientExplainer\"\"\"\n",
    "    logger.info(\"Computing SHAP values\")\n",
    "    # Process audio\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    input_values = inputs.input_values.to(device)\n",
    "    \n",
    "    # Ensure input_values has the correct shape [batch_size, sequence_length]\n",
    "    if len(input_values.shape) == 3:\n",
    "        input_values = input_values.squeeze(1)\n",
    "    \n",
    "    # Create background samples with correct shape [batch_size, sequence_length]\n",
    "    num_background = 5\n",
    "    background = torch.zeros((num_background, input_values.shape[1]), device=device)\n",
    "    background += torch.randn_like(background) * 0.01  # Add small random noise\n",
    "    logger.info(f\"Created background samples with shape {background.shape}\")\n",
    "    logger.info(f\"Background mean: {torch.mean(background).item():.6f}\")\n",
    "    logger.info(f\"Background std: {torch.std(background).item():.6f}\")\n",
    "    \n",
    "    # Initialize GradientExplainer with the model only\n",
    "    explainer = shap.GradientExplainer(\n",
    "        wrapped_model,\n",
    "        background,\n",
    "        batch_size=1\n",
    "    )\n",
    "    \n",
    "    # Log model output before SHAP computation\n",
    "    with torch.no_grad():\n",
    "        model_output = wrapped_model(input_values)\n",
    "        logger.info(f\"Model output shape: {model_output.shape}\")\n",
    "        logger.info(f\"Model output mean: {torch.mean(model_output).item():.6f}\")\n",
    "        logger.info(f\"Model output std: {torch.std(model_output).item():.6f}\")\n",
    "        logger.info(f\"Model output sum: {torch.sum(model_output).item():.6f}\")\n",
    "    \n",
    "    # retrieve logits\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "    # take argmax and decode\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    logger.info(f\"Predicted IDs: {predicted_ids}\")\n",
    "    transcription = processor.batch_decode(predicted_ids)\n",
    "    logger.info(f\"Transcription: {transcription}\")\n",
    "\n",
    "    output_string = ''.join([list(vocab.keys())[list(vocab.values()).index(id.item())] for id in predicted_ids[0]])\n",
    "    logger.info(f\"Decoded output string: {output_string}\")\n",
    "\n",
    "    # Get SHAP values\n",
    "    logger.info(\"Computing SHAP values with GradientExplainer\")\n",
    "    shap_values = explainer.shap_values(input_values)\n",
    "    logger.info(f\"Raw SHAP values type: {type(shap_values)}\")\n",
    "    \n",
    "    # Convert to numpy and process\n",
    "    # if isinstance(shap_values[0], torch.Tensor):\n",
    "    #     shap_values = [v.cpu().numpy() for v in shap_values]\n",
    "    \n",
    "    # Convert to numpy array and handle shapes\n",
    "    # shap_values = np.array(shap_values)  # Shape: (1, batch, seq_len)\n",
    "    logger.info(f\"SHAP values shape after conversion: {shap_values.shape}\")\n",
    "    \n",
    "    return shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a8f109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(processor, device, wrapped_model, model, vocab, test_set: List[Dict]) -> Dict:\n",
    "    \"\"\"Compute evaluation metrics for the test set\"\"\"\n",
    "    logger.info(\"Computing metrics for test set\")\n",
    "    metrics = {\n",
    "        \"shap_noise_correlation\": [],\n",
    "        \"shap_confidence_correlation\": [],\n",
    "        \"wer_correlation\": []\n",
    "    }\n",
    "    \n",
    "    # Store SHAP values and other computed values for visualization\n",
    "    visualization_data = []\n",
    "    \n",
    "    for i, sample in enumerate(tqdm(test_set, desc=\"Computing metrics\")):\n",
    "        logger.info(f\"Processing sample {i+1}/{len(test_set)}\")\n",
    "        audio = sample[\"audio\"]\n",
    "        text = sample[\"text\"]\n",
    "        # Get model prediction and confidence\n",
    "        inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "        input_values = inputs.input_values.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(input_values).logits\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            confidence = torch.mean(torch.max(probs, dim=-1)[0]).item()\n",
    "        logger.info(f\"Model confidence: {confidence:.4f}\")\n",
    "        \n",
    "        # Compute SHAP values\n",
    "        shap_values = compute_shap_values(processor, device, wrapped_model, model, vocab, audio)\n",
    "        logger.info(f\"SHAP values shape: {shap_values.shape}\")\n",
    "        logger.info(f\"SHAP values range: [{np.min(shap_values):.4f}, {np.max(shap_values):.4f}]\")\n",
    "\n",
    "        np.save(f\"data/shap_values_sample_{i+1}_{sample['type']}_{sample['snr']}\",shap_values)\n",
    "        np.save(f\"data/audio_sample_{i+1}_{sample['type']}_{sample['snr']}\",sample[\"audio\"])\n",
    "        np.save(f\"data/noise_sample_{i+1}_{sample['type']}_{sample['snr']}\",sample[\"noise\"])\n",
    "        np.save(f\"data/text_sample_{i+1}_{sample['type']}_{sample['snr']}.npy\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f489c480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-15 13:03:39,077 - INFO - Using device: cuda\n",
      "2025-09-15 13:03:39,078 - INFO - Loading model: facebook/wav2vec2-base-960h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-15 13:03:40,785 - INFO - Model loaded successfully\n",
      "2025-09-15 13:03:40,786 - INFO - Model wrapper created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Load model and processor\n",
    "logger.info(f\"Loading model: {model_name}\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "vocab = {\"<pad>\": 0, \"<s>\": 1, \"</s>\": 2, \"<unk>\": 3, \"|\": 4, \"E\": 5, \"T\": 6, \"A\": 7, \"O\": 8, \"N\": 9, \"I\": 10, \"H\": 11, \"S\": 12, \"R\": 13, \"D\": 14, \"L\": 15, \"U\": 16, \"M\": 17, \"W\": 18, \"C\": 19, \"F\": 20, \"G\": 21, \"Y\": 22, \"P\": 23, \"B\": 24, \"V\": 25, \"K\": 26, \"'\": 27, \"X\": 28, \"J\": 29, \"Q\": 30, \"Z\": 31}\n",
    "logger.info(\"Model loaded successfully\")\n",
    "\n",
    "# Create model wrapper for SHAP\n",
    "wrapped_model = _create_model_wrapper(model)\n",
    "logger.info(\"Model wrapper created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97d5cf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-15 13:03:40,792 - INFO - Creating test set...\n",
      "2025-09-15 13:03:40,792 - INFO - Creating test set with 3 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating test samples: 100%|██████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-15 13:03:42,953 - INFO - Test set created with 0 total samples\n",
      "2025-09-15 13:03:42,954 - INFO - []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize evaluator\n",
    "\n",
    "\n",
    "# Create test set\n",
    "logger.info(\"Creating test set...\")\n",
    "test_set = create_test_set(num_samples=3)\n",
    "logger.info(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc2be844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "print(len(test_set))\n",
    "sample = test_set[0]\n",
    "np.save(f\"data/audio_sample_{0}_{sample['type']}_{sample['snr']}\",sample[\"audio\"])\n",
    "audio = np.load(\"data/audio_sample_0_clean_inf.npy\")\n",
    "sf.write(\"data/test_audio.wav\",audio, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37e80624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-15 13:03:46,062 - INFO - Computing metrics...\n",
      "2025-09-15 13:03:46,063 - INFO - Computing metrics for test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing metrics: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics and get visualization data\n",
    "logger.info(\"Computing metrics...\")\n",
    "compute_metrics(processor, device, wrapped_model, model, vocab, test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
